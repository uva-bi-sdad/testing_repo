{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDAD Broadbandnow.com Scraper Code\n",
    "### Last Edit: 2/8/22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires addresses from Corelogic/similar source at the block level with 2 important columns: geoid_blk and mail_address. Original approach uses one address per block group and first generates cleaned list of addresses and block groups from this. Then, proceeds to scrape all package information for those selected addresses. Reports results at block group, tract, and county level within dataframe, which can be exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# generic imports\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# selenium imports\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if xpath exists, if not return false\n",
    "def check_exists_by_xpath(driver, xpath):\n",
    "    '''\n",
    "    Description:\n",
    "        Check existence of xpath on page\n",
    "    \n",
    "    Inputs:\n",
    "        webdriver: your webdriver\n",
    "        xpath: whatever element we are looking for\n",
    "        \n",
    "    Outputs:\n",
    "        returns True if xpath exists, False if not\n",
    "    '''\n",
    "    # try to find element\n",
    "    try:\n",
    "        driver.find_element_by_xpath(xpath)\n",
    "    \n",
    "    # throw exception and return false if unable to find\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Corelogic provides BLOCK level FIPs code, but we use BLOCK GROUP level data here\n",
    "def read_and_clean_addresses_for_bgs(data, need_subset = True, size_subset = 3):\n",
    "    '''\n",
    "    Description:\n",
    "        Check existence of xpath on page\n",
    "    \n",
    "    Inputs:\n",
    "        data: string, name of csv you want to use (includes .csv ending)\n",
    "        need_subset: boolean, True if using subset of data (originally used 1st address within each bg from list of 3) - default = True\n",
    "        size_subset: integer, if subsetting, selects every \"nth\" row (not necessary to mess with this param if using 1 address per bg) - default = 3\n",
    "        \n",
    "    Outputs:\n",
    "        returns True if xpath exists, False if not\n",
    "    '''\n",
    "    # read in csv, drop index, and update block column\n",
    "    address_sample_3_per_bg = pd.read_csv(data, index_col = 0)\n",
    "    address_sample_3_per_bg = address_sample_3_per_bg.reset_index(drop = True)\n",
    "    address_sample_3_per_bg['geoid_blk'] = address_sample_3_per_bg.geoid_blk.astype(str)\n",
    "    \n",
    "    # drop lat 4 digits of mail address to get short zipcode\n",
    "    a = address_sample_3_per_bg.mail_address.values\n",
    "    a = np.array([a[i][0:-4] if a[i][-9].isdigit() else a[i] for i in range(len(a))])\n",
    "    \n",
    "    # get block group geoid\n",
    "    address_sample_3_per_bg['geoid_bg'] = address_sample_3_per_bg.geoid_blk.str.slice(start=0, stop=12)\n",
    "    \n",
    "    # if data needs subsetting (I had 3 addresses )\n",
    "    if need_subset:\n",
    "        addresses = a[::size_subset]\n",
    "        block_geoids = address_sample_3_per_bg.geoid_bg[::size_subset]\n",
    "        \n",
    "    else:\n",
    "        addresses = a\n",
    "        block_geoid = address_sample_3_per_bg.geoid_bg\n",
    "    \n",
    "    return addresses, block_geoids.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_address2(address, driver, driver_wait = 20):\n",
    "    '''\n",
    "    Description:\n",
    "        Check existence of xpath on page\n",
    "    \n",
    "    Inputs:\n",
    "        address: string, single home address we are scraping for\n",
    "        driver: your webdriver\n",
    "        driver_wait: integer, wait time for driver - default = 20\n",
    "        \n",
    "    Outputs:\n",
    "        returns True if xpath exists, False if not\n",
    "    '''\n",
    "    # wait until search bar is clickable and enter address\n",
    "    wait = WebDriverWait(driver, driver_wait)\n",
    "    search = wait.until(EC.element_to_be_clickable((By.ID, 'plan-search')))\n",
    "    search.clear()\n",
    "    search.send_keys(\"{}\".format(address))\n",
    "\n",
    "    # sleep, then go to top suggested address\n",
    "    time.sleep(sleep_time)\n",
    "    go_top = check_exists_by_xpath(driver, '//*[@id=\"plans-search\"]/div/div/div[1]/div/div/div/ul')\n",
    "\n",
    "    # click top address\n",
    "    if go_top:\n",
    "        go_top_address = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"plans-search\"]/div/div/div[1]/div/div/div/ul/li')))\n",
    "        go_top_address.click()\n",
    "        \n",
    "    return go_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_prices(driver, addresses):\n",
    "    '''\n",
    "    Description:\n",
    "        Scrape internet packages from Broadbandnow.com - takes each address and scrapes all packages for top match\n",
    "    \n",
    "    Inputs:\n",
    "        driver: your webdriver\n",
    "        addresses: array of strings, home addresses we are scraping for (first output of read_and_clean_addresses_for_bgs)\n",
    "        \n",
    "    Outputs:\n",
    "        all_prices: jagged list (list of varying sized lists), package prices\n",
    "        all_names: jagged list (same size as all_prices), package names\n",
    "        all_type_list: jagged list (same size as all_prices), package names\n",
    "        all_speeds: jagged list (same size as all_prices), package names\n",
    "        idxs: array, array of indices where information was successfully scraped (aligns with addresses)\n",
    "    '''\n",
    "    # create empty lists for prices, names, speeds, and types - will become jagged lists (lists of varying sized lists)\n",
    "    all_prices = []\n",
    "    all_names = []\n",
    "    all_speeds = []\n",
    "    all_type_list = []\n",
    "    idxs = []\n",
    "\n",
    "    # initialize variables and get start time\n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    problem_counter = 0\n",
    "\n",
    "    # loop over block group addressed\n",
    "    while i < len(addresses):\n",
    "        # try below and exception IF takes too long (increments a counter before skipping address eventually)\n",
    "        try:\n",
    "            # reload page to clear results (noticed that we run into issues if we do not clear)\n",
    "            driver.get(\"https://broadbandnow.com/compare/plans\")\n",
    "            go_top = search_address2(addresses[i], driver)\n",
    "\n",
    "            # select top address\n",
    "            if go_top:\n",
    "                time.sleep(1)\n",
    "                unable_to_confirm = check_exists_by_xpath(driver, \"/html/body/div[2]/div/div/div[1]/section/section/div/div/div[1]/div/section\")\n",
    "\n",
    "                # if able to confirm and go to top address\n",
    "                if not unable_to_confirm:\n",
    "                    #\n",
    "                    time.sleep(1)\n",
    "                    load_more = check_exists_by_xpath(driver, '//*[@id=\"cityPlansListing\"]/section/div/div[2]/div/div/section')\n",
    "\n",
    "                    #if load more is an option, then load all packages\n",
    "                    if load_more:\n",
    "                        # load all plans\n",
    "                        load_all_plans = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"cityPlansListing\"]/section/div/div[2]/div/div/section')))\n",
    "                        load_all_plans.click()\n",
    "\n",
    "                    # bs - scrape page\n",
    "                    html = driver.page_source\n",
    "                    soup = BeautifulSoup(html)\n",
    "\n",
    "                    # extract list of prices\n",
    "                    price_temp_list = soup.find_all(attrs = {\"class\": \"c-provider-card__plan-value\"})\n",
    "                    price = np.array([float(price_temp_list[i].getText().split(\"$\")[-1]) for i in range(len(price_temp_list))])\n",
    "\n",
    "                    # extract list of name of provider\n",
    "                    name_temp_list = soup.find_all(attrs = {\"class\": \"c-provider-card__provider-name\"})\n",
    "                    name = np.array([name_temp_list[i].getText().split(\". \")[1] for i in range(len(name_temp_list))])\n",
    "\n",
    "                    # extract list of speeds\n",
    "                    speed_temp_list = soup.find_all(attrs = {\"class\": \"c-provider-card__speeds-value\"})\n",
    "                    speed = np.array([float(speed_temp_list[i].getText().split(\" \")[0]) for i in range(len(speed_temp_list))])\n",
    "\n",
    "                    # extract string - \"Upload\" or \"Download\"\n",
    "                    down_up_temp_list = soup.find_all(attrs = {\"class\": \"c-provider-card__speeds-label\"})\n",
    "                    down_up = np.array([down_up_temp_list[i].getText() for i in range(len(speed_temp_list))])\n",
    "\n",
    "                    # extract type of internet service\n",
    "                    type_temp_list = soup.find_all(attrs = {\"class\": \"c-provider-card__label\"})\n",
    "                    type_list = np.array([type_temp_list[i].getText().strip() for i in range(len(type_temp_list))])\n",
    "\n",
    "                    # create empty 2D array for speeds \n",
    "                    speed_array = np.zeros([np.sum(down_up == \"Download\"), 2]) * np.nan\n",
    "                    \n",
    "                    # set counter to 0, will denote the row in speed_array we are filling in\n",
    "                    count = 0\n",
    "\n",
    "                    # loop over packages listed\n",
    "                    for k in range(len(down_up)):\n",
    "\n",
    "                        # if download speed\n",
    "                        if down_up[k] == \"Download\":\n",
    "                            if k != 0:\n",
    "                                count += 1\n",
    "\n",
    "                            # add download speed\n",
    "                            speed_array[count, 0] = speed[k]\n",
    "\n",
    "                        # if upload, add upload speed\n",
    "                        else:\n",
    "                            speed_array[count, 1] = speed[k]\n",
    "\n",
    "                    # select edit option to change address\n",
    "                    edit = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"plans-search\"]/div/div/div/h1/span')))\n",
    "                    edit.click()\n",
    "\n",
    "                    # append to lists\n",
    "                    idxs.append(i)\n",
    "                    all_prices.append(price)\n",
    "                    all_names.append(name)\n",
    "                    all_type_list.append(type_list)\n",
    "                    all_speeds.append(speed_array)\n",
    "\n",
    "                    # set problem counter \n",
    "                    problem_counter = 0\n",
    "            \n",
    "            # increment address counter within while loop\n",
    "            i += 1\n",
    "\n",
    "        # if try fails, throw exception and increment counter (retry until problem_counter hits 5)\n",
    "        # throws error if we try to edit search plans but this is not an option because nothing was searched after hitting home page\n",
    "        except TimeoutException as ex:\n",
    "            problem_counter += 1\n",
    "            \n",
    "            # if 2 problems with address, increment address counter, skip address, and reset problem counter\n",
    "            if problem_counter == 2:\n",
    "                i += 1\n",
    "                print(\"skip\")\n",
    "                problem_counter = 0\n",
    "\n",
    "        # get time taken to run as well as % completetion\n",
    "        mid = time.time()\n",
    "        if i == 1 * int(len(addresses)/10): print(\"10% @ {}\".format(mid - start))\n",
    "        if i == 2 * int(len(addresses)/10): print(\"20% @ {}\".format(mid - start))\n",
    "        if i == 3 * int(len(addresses)/10): print(\"30% @ {}\".format(mid - start))\n",
    "        if i == 4 * int(len(addresses)/10): print(\"40% @ {}\".format(mid - start))\n",
    "        if i == 5 * int(len(addresses)/10): print(\"50% @ {}\".format(mid - start))\n",
    "        if i == 6 * int(len(addresses)/10): print(\"60% @ {}\".format(mid - start))\n",
    "        if i == 7 * int(len(addresses)/10): print(\"70% @ {}\".format(mid - start))\n",
    "        if i == 8 * int(len(addresses)/10): print(\"80% @ {}\".format(mid - start))\n",
    "        if i == 9 * int(len(addresses)/10): print(\"90% @ {}\".format(mid - start))\n",
    "\n",
    "    # close driver\n",
    "    driver.quit()        \n",
    "    \n",
    "    # convert indices to array and get time\n",
    "    idxs = np.array(idxs)\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    return all_prices, all_names, all_type_list, all_speeds, idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten list of lists\n",
    "def flatten(t):\n",
    "    '''\n",
    "    Description:\n",
    "        Flattens our lists of lists so that we can make clean dataframe; helper function\n",
    "    \n",
    "    Inputs:\n",
    "        t: list of lists (jagged list)\n",
    "        \n",
    "    Outputs:\n",
    "         np.array([item for sublist in t for item in sublist]): array, flattened array from list of lists\n",
    "    '''\n",
    "    return np.array([item for sublist in t for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(all_prices, all_names, all_type_list, all_speeds, idxs):\n",
    "    '''\n",
    "    Description:\n",
    "        Make dataframe using prices, names, internet types, speeds, and FIPs code at bg, tr, and ct levels\n",
    "    \n",
    "    Inputs:\n",
    "        all_prices: jagged list (list of varying sized lists), package prices\n",
    "        all_names: jagged list (same size as all_prices), package names\n",
    "        all_type_list: jagged list (same size as all_prices), package names\n",
    "        all_speeds: jagged list (same size as all_prices), package names\n",
    "        idxs: array, array of indices where information was successfully scraped (aligns with addresses)\n",
    "        \n",
    "    Outputs:\n",
    "        all_prices: jagged list (list of varying sized lists), package prices\n",
    "        all_names: jagged list (same size as all_prices), package names\n",
    "        all_type_list: jagged list (same size as all_prices), package names\n",
    "        all_speeds: jagged list (same size as all_prices), package names\n",
    "        idxs: array, array of indices where \n",
    "    '''\n",
    "    # get number of packages, valid address, and construct result (the addresses column in the dataframe)\n",
    "    num_packages = np.array([len(all_prices[i]) for i in range(len(all_prices))])\n",
    "    valid_addresses = addresses[idxs]\n",
    "    \n",
    "    # repeat a valid address \"num_packages\" times\n",
    "    result = np.array([valid_addresses[i] for i in range(len(num_packages)) for j in range(num_packages[i])])\n",
    "    \n",
    "    # flatten download and upload arrays\n",
    "    download = flatten([all_speeds[i][:, 0] for i in range(len(all_speeds))])\n",
    "    upload = flatten([all_speeds[i][:, 1] for i in range(len(all_speeds))])\n",
    "\n",
    "    # get block groups, tracts, and counties from addresses data and add to dataframe\n",
    "    short_blockgroup_geoid = block_geoids[idxs]\n",
    "    short_tract_geoid = np.array([x[:11] for x in short_blockgroup_geoid])\n",
    "    short_county_geoid = np.array([x[:5] for x in short_blockgroup_geoid])\n",
    "\n",
    "    # repeat block group, tract, county name \"num_packages\" times for the number of packages within that area (these will be columns in df)\n",
    "    short_blockgroup_geoid2 = np.array([short_blockgroup_geoid[i] for i in range(len(num_packages)) for j in range(num_packages[i])])\n",
    "    short_tract_geoid2 = np.array([short_tract_geoid[i] for i in range(len(num_packages)) for j in range(num_packages[i])])\n",
    "    short_county_geoid2 = np.array([short_county_geoid[i] for i in range(len(num_packages)) for j in range(num_packages[i])])\n",
    "\n",
    "    # final dataframe\n",
    "    df = pd.DataFrame({\"address\": result, \"price\": flatten(all_prices), \"name\": flatten(all_names),\n",
    "                       \"type\": flatten(all_type_list), \"download\": download, \"upload\": upload,\n",
    "                       \"block_group\": short_blockgroup_geoid2, \"tract\": short_tract_geoid2,\n",
    "                       \"county\": short_county_geoid2})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 98.0.4758\n",
      "Get LATEST driver version for 98.0.4758\n",
      "Driver [/Users/zacharyalerte/.wdm/drivers/chromedriver/mac64/98.0.4758.80/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "# start driver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(\"https://broadbandnow.com/compare/plans\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# set driver params\n",
    "driver_wait = 20\n",
    "sleep_time = 2\n",
    "wait = WebDriverWait(driver, driver_wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run read_and_clean_addresses_for_bgs and get list of addresses, block_geoids\n",
    "# NEED: list of addresses with GEOID for block as \"geoid_blk\", mail address as \"mail_address\"\n",
    "# I used Corelogic for housing information and scraped 3 for each block group, although I only use 1\n",
    "addresses, block_geoids = read_and_clean_addresses_for_bgs(data = \"three_address_in_block_group.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10% @ 46.869096994400024\n",
      "20% @ 94.06219601631165\n",
      "30% @ 137.4705250263214\n",
      "40% @ 183.8766860961914\n",
      "50% @ 230.95758199691772\n",
      "60% @ 276.1324129104614\n",
      "70% @ 324.4493188858032\n",
      "80% @ 371.9722089767456\n",
      "90% @ 423.3359649181366\n",
      "471.25773215293884\n"
     ]
    }
   ],
   "source": [
    "# given addresses in the correct format and driver: gets prices, names, types, speeds, (and indicies where successful)\n",
    "all_prices, all_names, all_type_list, all_speeds, idxs = scrape_prices(driver, addresses[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produces dataframe\n",
    "df = make_df(all_prices, all_names, all_type_list, all_speeds, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>price</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>download</th>\n",
       "      <th>upload</th>\n",
       "      <th>block_group</th>\n",
       "      <th>tract</th>\n",
       "      <th>county</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138  ROBINSON RD  HAMPTON VA 23661</td>\n",
       "      <td>89.99</td>\n",
       "      <td>Verizon Fios</td>\n",
       "      <td>Fios Gigabit Connection</td>\n",
       "      <td>940.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>516500115004</td>\n",
       "      <td>51650011500</td>\n",
       "      <td>51650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138  ROBINSON RD  HAMPTON VA 23661</td>\n",
       "      <td>99.99</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td>Internet Gigablast</td>\n",
       "      <td>940.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>516500115004</td>\n",
       "      <td>51650011500</td>\n",
       "      <td>51650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138  ROBINSON RD  HAMPTON VA 23661</td>\n",
       "      <td>154.99</td>\n",
       "      <td>Verizon Fios</td>\n",
       "      <td>Fios Gigabit Connection + Fios TV Test Drive</td>\n",
       "      <td>940.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>516500115004</td>\n",
       "      <td>51650011500</td>\n",
       "      <td>51650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138  ROBINSON RD  HAMPTON VA 23661</td>\n",
       "      <td>189.99</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td>Internet Gigablast + Contour TV Preferred</td>\n",
       "      <td>940.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>516500115004</td>\n",
       "      <td>51650011500</td>\n",
       "      <td>51650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138  ROBINSON RD  HAMPTON VA 23661</td>\n",
       "      <td>209.99</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td>Internet Gigablast + Contour TV Preferred + Vo...</td>\n",
       "      <td>940.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>516500115004</td>\n",
       "      <td>51650011500</td>\n",
       "      <td>51650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              address   price                name  \\\n",
       "0  138  ROBINSON RD  HAMPTON VA 23661   89.99        Verizon Fios   \n",
       "1  138  ROBINSON RD  HAMPTON VA 23661   99.99  Cox Communications   \n",
       "2  138  ROBINSON RD  HAMPTON VA 23661  154.99        Verizon Fios   \n",
       "3  138  ROBINSON RD  HAMPTON VA 23661  189.99  Cox Communications   \n",
       "4  138  ROBINSON RD  HAMPTON VA 23661  209.99  Cox Communications   \n",
       "\n",
       "                                                type  download  upload  \\\n",
       "0                            Fios Gigabit Connection     940.0   880.0   \n",
       "1                                 Internet Gigablast     940.0    35.0   \n",
       "2       Fios Gigabit Connection + Fios TV Test Drive     940.0   880.0   \n",
       "3          Internet Gigablast + Contour TV Preferred     940.0    35.0   \n",
       "4  Internet Gigablast + Contour TV Preferred + Vo...     940.0    35.0   \n",
       "\n",
       "    block_group        tract county  \n",
       "0  516500115004  51650011500  51650  \n",
       "1  516500115004  51650011500  51650  \n",
       "2  516500115004  51650011500  51650  \n",
       "3  516500115004  51650011500  51650  \n",
       "4  516500115004  51650011500  51650  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
